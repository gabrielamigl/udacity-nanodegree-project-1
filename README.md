# Data Engineering Nano degree - PostgreSQL Data Warehouse

For this project, we were requested to create an ETL project to read JSON files containing data about users and music listening from Sparkify. A data warehouse structure was requested using ***star*** schema.
This data warehouse could have many uses, like identify users patterns and provide summarised data.

## Project Execution

The right order to execute this project is:
1. `create_tables.py`
2. `etl.py`

The scripts have to be executed on this order for successfully execution. The `test.ipynb` notebook is of optional execution and should be executed after the whole process is finished.

## Files Structure
### Scripts
#### `sql_queries.py`
This script is in charge of the SQL statements definition. It has to be imported by others script in order to load the scripts. 
Some queries are stored on arrays according to its own use. Two arrays are created: one for tables creation and another one for tables dropping. 

#### `create_tables.py`
This script is the one in charge of the database creation and tables dropping and creation. The following functions can be found on the script: 

`create_database()` creates a connection with the PostgreSQL instance, drops the database sparkifydb and recreates the database. 

`drop_tables(cur, conn)` receives the connection and cursor and execute the scripts that are included on the array  drop_table_queries that is generated by the `sql_queries.py` script.

`create_tables(cur, conn)` also accepts as parameters the cursor and the database connection and executes the creation of table statements. These scripts are included on create_table_queries array that was defined on `sql_queries.py`.


#### `etl.py`
This is the script that executes the ETL job logic itself. Its job is to iterate the files according to the data directories given, load the JSON files, process the data and save the result on the database. The following functions are found on the script.

`process_data(cur, conn, filepath, func)` requires the following parameters: database cursor and connection, filepath and function to be applied on each file, as it could vary according to the type of the data. The filepath is not a absolute path but a folder that may contains songs or log data.

The function will iterate on the given directory and invoke the given function, that could be `process_song_file` or `process_log_file`. In order to follow the iteration, some messages are printed on the console to identify the total number of files to iterate and the current file.

`process_song_file(cur, filepath)` requires as parameters the database cursor and the absolute filepath. The purpose of this function is simply to read the JSON file that is provided, split artist and song data and insert the data on the database.



`process_log_file(cur, filepath)` requires as parameters the database cursor and the absolute filepath. This table  will read the data from the JSON file, do some data manipulation and populate three tables: users, times and songplays.


### Notebooks - extra

#### `etl.ipynb`
That's just a notebook to explore the operation of the ETL process from a single file point of view. It's not a mandatory execution.

#### `test.ipynb`
This notebook's goal is to check the data that is stored on the database and it's not a necessary execution. If desired to check the records, it should be executed after the ETL process is finished.

## Tables 


#### **songplays**
This table stores information about songs that were executed by Sparkify users. For each song that was played, a record is created. That's the fact table. The primary key is the songplay_id which is going to be a serial ID

#### **users**
That is the users' dimension table, where will be stored important info like User ID, Name and Gender are going to seat here. The primary key is the user_id, which is provided on the JSON files.

#### **songs**
This table stores information about the songs - It's a dimension. with information about songs. The primary key will be the song_id, provided by the JSON files.

#### **artists**
That table stores artists' data and will be directly by songs. It's a dimension and the primary key is going to be the artist_id, information that is included on the Sparkify's JSON files.

#### **time**
That's another dimension table. It contains detailed information about every timestamp present on *songplays* table. The primary key is going to be start_time.


